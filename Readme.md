# PDF Document Processing and Q&A System

A Python tool for converting PDFs to Markdown and asking questions about document content using either OpenAI or a local AI server (Ollama). This repo contains a small wrapper class (ThriveAiDocling) and an example runner (main.py).

## Features
- Convert local or online PDFs to Markdown (via docling)
- Ask questions about document content using:
  - OpenAI (cloud) or
  - Local AI server (Ollama / local endpoint)
- Simple verification flow that scores answers and optionally caches high-confidence Q/As

## Repository layout
- `thrive_ai.py` — main wrapper `ThriveAiDocling` (PDF conversion, QA, verification, embedding)
- `main.py` — minimal example showing conversion, saving/loading Markdown, and asking questions
- `requirements.txt` — Python dependencies (see below)
- `memory.json` — generated by the class to store verified Q/As (created at runtime)

## Prerequisites

### System
- macOS (examples below)
- Python 3.10+ recommended (3.11 Works Best)
- If using local model tooling (Ollama), follow its install instructions (see "Local AI (Ollama)" section).
- The original README referenced some Linux libraries (libgl1-mesa-glx, libglib2.0-0) — those are for Linux systems only.

### Python / Virtual environment (Optional)
Create and activate a venv (macOS example):
```bash
python3 -m venv .venv
source .venv/bin/activate
```

## Installation

Install Python dependencies from the repo `requirements.txt` (adjust path if different):
```bash
python -m pip install --upgrade pip
python -m pip install -r requirements.txt
```

If you prefer, install key packages manually:
```bash
python -m pip install docling openai requests numpy opencv-python
```

## Local AI (Ollama) — optional
The code supports a local server endpoint at `http://localhost:11434/api/generate` (used by `ask_document_question_free`).

Install Ollama (example) and pull a model:
```bash
# follow Ollama install per their docs; example:
curl -fsSL https://ollama.ai/install.sh | sh

# Pull a model (example used in code)
ollama pull llama3.2:3b
```

The local call in `thrive_ai.py` posts a JSON body to `http://localhost:11434/api/generate` with keys `model`, `prompt`, and `stream`.

## OpenAI (cloud) — optional
If you want to use the OpenAI-based functions (`ask_document_question`, `embed`, verification with Chat API), export your API key:
```bash
export OPENAI_API_KEY="sk-..."
```
The code uses the new OpenAI SDK style (from `openai import OpenAI`) and references models like `"gpt-5-nano"` and `"text-embedding-3-small"`. Adjust model names to ones available in your account.

## How to use

1. Convert a PDF to Markdown and save it (example in `main.py`)

- Open `main.py` and set `processing_docs = True` to convert and save a PDF:
```python
from thrive_ai import ThriveAiDocling

docling = ThriveAiDocling()
processing_docs = True

if processing_docs:
    online_context = docling.process_pdf("https://arxiv.org/pdf/2408.09869")
    docling.save_markdown_to_file(online_context, "output.md")
```

Run:
```bash
python /Users/jakemack/Desktop/src/local/docling/main.py
```

2. Ask a question about a saved document (example from `main.py`)

- Default example in `main.py` loads `output.md`, asks a question, and prints the answer and verification score:
```python
question = "Is the model pipeline extensible ?"
online_context = docling.load_markdown_from_file("output.md")
answer, verification, gen = docling.answer_question(question, online_context)

print("GENERATION TYPE:", gen)
print("\nVERIFICATION SCORE:\n", verification['score'])
print("ANSWER:\n", answer)
```

3. Using local vs OpenAI endpoints
- `ask_document_question` uses the OpenAI SDK and requires `OPENAI_API_KEY`.
- `ask_document_question_free` sends a request to `http://localhost:11434/api/generate` and returns the `response` field; ensure the local server is running and the model matches.

4. Memory / caching
- High-confidence answers (score >= 85) are saved to `memory.json` via `add_verified_item`.
- `answer_question` will first try to find a matching verified answer using embeddings.

## Example commands

Activate venv and run the example:
```bash
source .venv/bin/activate
python -m pip install -r /Users/jakemack/Desktop/src/local/docling/requirements.txt
export OPENAI_API_KEY="sk-..."
python /Users/jakemack/Desktop/src/local/docling/main.py
```

If using local Ollama server:
```bash
# start ollama / ensure model is loaded, then run
python /Users/jakemack/Desktop/src/local/docling/main.py
```

## Notes & troubleshooting
- If you hit model or API issues with OpenAI, confirm your API key and available model names.
- If local generation fails, check that the local server is reachable at `http://localhost:11434` and that the model name matches what you pulled.
- `docling` may have additional system-level dependencies for PDF parsing or OpenCV. On macOS those are typically satisfied via pip, but you may need to install Homebrew packages for complex environments.

## Quick API reference (ThriveAiDocling)
- process_pdf(source: str) -> markdown string
- save_markdown_to_file(content: str, output_file: str)
- load_markdown_from_file(file_path: str) -> str
- ask_document_question(question, markdown_content) -> str (OpenAI)
- ask_document_question_free(question, markdown_content) -> str (local server)
- answer_question(question, markdown_content) -> (answer, verification_dict, generation_type)
- verification_prompt(question, answer, document_text) -> dict (verification JSON)
- embed(text) -> embedding vector
- search_memory(query_embedding, memory, threshold=0.85)

If you want, I can produce a short example README section tailored to macOS with Homebrew commands to install any additional system packages.// filepath: /Users/jakemack/Desktop/src/local/docling/Readme.md
